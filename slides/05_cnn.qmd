## Convolutional neural networks 

<br>

- Regular NN don't scale well to images
- CNNs take advantage of the fact that their inputs are images

::: {.r-stack}
::: {.fragment .fade-in-then-out}
<div style="text-align: right; margin: 0 auto;">
![](../img/cnn/cnn-transforms.png){}
<div style="font-size: 0.3em; color: var(--link-color); margin-top: -17px;">
Figure from <a href="https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote20.pdf" target="_blank">
CS4780 lecture notes</a>
</div>
</div>

:::


::: {.fragment .fade-in}
<div style="text-align: right; width: 80%; margin: 0 auto;">
![](../img/overview/cnn-diagram-1.png){}
<div style="font-size: 0.3em; color: var(--link-color); margin-top: -17px;">
Figure from <a href="https://cs231n.github.io/convolutional-networks" target="_blank">
cs231n.github.io/convolutional-networks</a>
</div>
</div>

:::
:::


::: {.notes}

- Regular NNs don't scale well to images
  - An image of size 200x200x3 would mean a first hidden layer with 120,000 weights per neuron!
  - The full connectivity is overkill

- CNNs take advantage of the fact that their inputs are images
  - this means they can make sensible modifications to the architecture. 
  
  The modifications to the architecture encode the assumptions of the functions that we want to fit. 
  
  For example, if we want to train a network to fit the function that detects if an image contains a cat, when we use a CNN we constrain that space of possible functions to only those that are translation invariant, just by using this architecture.

We can see them as subsequent transformations of the input image.

We can also see their neurons as being 3-dimensional, with width, height and depth (if like before, we consider the neurons to hold the outputs of the computations)

> A simple ConvNet is a sequence of layers, and every layer of a ConvNet transforms one volume of activations [neuron outputs] to another
:::



## Layers used in CNNs

Three common types of layers:

- Convolutional layer
- Pooling layer
- Batch normalisation layer
- Fully connected layer

::: {.notes}
Fully connected layers are the same as in regular NNs.
:::

## Layers used in CNNs

Three common types of layers:

:::{.nonincremental}
- **Convolutional layer**
- Pooling layer
- Batch normalisation layer
- Fully connected layer
:::

::: {.notes}
We will focus on the convolutional layer.
:::



## Convolutional layer

::: {.columns}
::: {.column .nonincremental style="width: 50%;"}

:::{.fragment .fade-in data-fragment-index="0"}
- A set of _n_ learnable **filters** 
:::

:::{.fragment .fade-in data-fragment-index="1"}
- Each filter is a small matrix of weights + 1 bias
:::

:::{.fragment .fade-in data-fragment-index="2"}
- We slide (convolve) each filter across the width and height of the input volume and **take the full depth**
:::

:::{.fragment .fade-in data-fragment-index="3"}
- Parameters are shared
:::

:::
:::

::: {.column style="width: 40%; margin-left: 5%;"}
:::{.r-stack}
:::{.fragment .fade-in data-fragment-index="0"}
:::{.fragment .fade-out data-fragment-index="2"}
<div style="text-align: right; width: 100%; margin: 0 auto;">
![](../img/cnn/convolution-n-channels.png){width=100%, fig-align="center"}
<div style="font-size: 0.3em; color: var(--link-color); margin-top: -17px;">
Figure modified from <a href="https://cs231n.github.io/convolutional-networks/" target="_blank">
CS231n</a>
</div>
</div>
:::
:::

:::{.fragment .fade-in data-fragment-index="2"}
:::{.fragment .fade-out data-fragment-index="3"}
<div style="text-align: right; width: 80%; margin: 0 auto;">
![](../img/cnn/padding_strides.gif){width=80%, fig-align="center" style="transform: rotate(40deg)"}
<div style="font-size: 0.3em; color: var(--link-color); margin-top: 50px;">
Figure from <a href="https://github.com/vdumoulin/conv_arithmetic" target="_blank">Convolution arithmetic</a>
</div>
</div>
:::
:::

:::{.fragment .fade-in data-fragment-index="3"}
<div style="text-align: right; width: 100%; margin: 0 auto;">
![](../img/cnn/convolution-sharing.png){width=100%, fig-align="center"}
<div style="font-size: 0.3em; color: var(--link-color); margin-top: -17px;">
Figure modified from <a href="https://cs231n.github.io/convolutional-networks/" target="_blank">
CS231n</a>
</div>
</div>
:::

:::
:::


::: {.notes}
- A set of _n_ learnable **filters** or **kernels**
- Each filter is a small matrix of weights
    - small along width and height but always extends to the full depth of the input
    - e.g. a filter on the first layer of a CNN may have size 5x5x3 (+ 1 bias parameter)

- In the forward pass:
    - we slide (convolve) each filter across the width and height of the input volume and compute dot products
    - we then would apply a non-linear activation function such as ReLU
    - the output is a 2D "activation map" that gives the responses of that filter at every position
    - for a layer with 12 filters, we will stack the 12 activation maps along depth to produce the output volume 

- We can formulate the forward pass as a matrix multiplication

Intuitively, the network will learn filters that activate when they see some type of visual feature such as an edge of some orientation or a blotch of some color on the first layer, or eventually entire honeycomb or wheel-like patterns on higher layers of the network.
:::

## Convolutional layer

:::{.columns}
:::{.column style="width: 50%;"}

A few hyperparameters:

:::{.fragment .fade-in .nonincremental data-fragment-index="0"}
- number of filters
- filter size
- stride
- padding

:::
:::

:::{.column style="width: 50%;"}

:::{.r-stack}
:::{.fragment .fade-in-then-out}
<div style="text-align: right; width: 80%; margin: 0 auto;">
![](../img/cnn/same_padding_no_strides.gif){width=80%, fig-align="center" style="transform: rotate(40deg)"}
<div style="font-size: 0.3em; color: var(--link-color); margin-top: 50px;">
Stride = 1. Figure from <a href="https://github.com/vdumoulin/conv_arithmetic" target="_blank">Convolution arithmetic</a>
</div>
</div>

:::

:::{.fragment .fade-in}
<div style="text-align: right; width: 80%; margin: 0 auto;">
![](../img/cnn/padding_strides.gif){width=80%, fig-align="center" style="transform: rotate(40deg)"}
<div style="font-size: 0.3em; color: var(--link-color); margin-top: 50px;">
Stride = 2. Figure from <a href="https://github.com/vdumoulin/conv_arithmetic" target="_blank">Convolution arithmetic</a>
</div>
</div>
:::

:::

:::
:::







:::{.notes}
There are a few hyperparameters involved in the convolutional layer:
- number of filters: the depth of the output volume
- filter size: the size of the filter (width and height)
- stride: the number of pixels that the filter moves by at each step
    - A stride larger than one will reduce the size of the output volume
- padding: the number of 0 pixels that are added to the input volume at each side
    - Padding is used to control the size of the output volume



:::


## Layers used in CNNs

Three common types of layers:

:::{.nonincremental}
- Convolutional layer
- Pooling layer
- Batch normalisation layer
- Fully connected layer
:::

::: {.notes}
Briefly explain pooling layer, batch norm. Fully connected layer is the same as in regular NNs.

> In other words, the most common ConvNet architecture follows the pattern:
INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC


> A simple ConvNet for CIFAR-10 classification could have the architecture [INPUT - CONV - RELU - POOL - FC]. 
- INPUT [32x32x3] will hold the raw pixel values of the image, in this case an image of width 32, height 32, and with three color channels R,G,B.
- CONV layer will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume. This may result in volume such as [32x32x12] if we decided to use 12 filters.
- RELU layer will apply an elementwise activation function, such as the max(0,x)
 thresholding at zero. This leaves the size of the volume unchanged ([32x32x12]).
POOL layer will perform a downsampling operation along the spatial dimensions (width, height), resulting in volume such as [16x16x12].
- FC (i.e. fully-connected) layer will compute the class scores, resulting in volume of size [1x1x10], where each of the 10 numbers correspond to a class score, such as among the 10 categories of CIFAR-10. As with ordinary Neural Networks and as the name implies, each neuron in this layer will be connected to all the numbers in the previous volume.
:::


<!-- ## Layers used in CNNs: pooling

- A convolutional max filter is applied to each depth slice independently
- 2 hyperparameters:
    - filter size
    - stride
- It resized the input in width and height
- No parameters to learn!

<div style="text-align: right; width: 80%; margin: 0 auto;">
![](../img/cnn/maxpooling2.png){width=80%, fig-align="center"}
<div style="font-size: 0.3em; color: var(--link-color); margin-top: -17px;">    
Figure modified from <a href="https://cs231n.github.io/convolutional-networks/" target="_blank">
CS231n</a>
</div>
</div>

::: {.notes}
- common to periodically insert a Pooling layer in-between successive Conv layers
- most common is max pooling, but also mean
- Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting.
::: -->




## Transfer learning

- Few people train a ConvNet from scratch

- Two common scenarios:
    - Fine-tuning (very common)
    - As a feature extractor (less common, e.g. [DINOv2](https://dinov2.metademolab.com/))

- Or simply run inference! (e.g. human pose estimation)


::: {.notes}
> In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest. 

You should look at whatever architecture currently works best on ImageNet, download a pretrained model and finetune it on your data. You should rarely ever have to train a ConvNet from scratch 

Fine tuning: use the pretrained model as a weight initialization
> fine-tune the weights of the pretrained network by continuing the backpropagation. It is possible to fine-tune all the layers of the ConvNet, or it’s possible to keep some of the earlier layers fixed (due to overfitting concerns) and only fine-tune some higher-level portion of the network.

As a feature extractor: to compute embeddings for your images
> Take a ConvNet pretrained on ImageNet, remove the last fully-connected layer (this layer’s outputs are the 1000 class scores for a different task like ImageNet), then treat the rest of the ConvNet as a fixed feature extractor for the new dataset. 

You may even be able to simply run inference on a pretrained model (e.g., human pose estimation)

:::


## Transfer learning

:::{.columns}
:::{.column style="width: 50%;"}
Common architectures:

- ResNet
- VGG
- Inception
- MobileNet
- EfficientNet
- UNet
- ...

:::

:::{.column style="width: 50%;"}

:::{.fragment .fade-in}
<div style="text-align: left; width: 100%; margin: 0 auto; margin-left: -10%; margin-top: 30%;">
![](../img/cnn/vgg16.png){width=100%, fig-align="center"}
<div style="font-size: 0.3em; color: var(--link-color); margin-top: -17px;">
Figure from <a href="https://www.neuralception.com/objectdetection-convnetbasics" target="_blank">
Neuralception</a>
</div>
</div>
:::

:::
:::



::: {.notes}
In animal pose estimation, these architectures are often mentioned as possible "backbones"

Head vs backbone
:::



## Data augmentation





:::{.notes}


:::








## Additional references
::: {.nonincremental style="font-size: 0.6em;"}
- CS231n: Convolutional Neural Networks
    - [https://cs231n.github.io/convolutional-networks/](https://cs231n.github.io/convolutional-networks/)
    - [https://cs231n.github.io/transfer-learning/](https://cs231n.github.io/transfer-learning/)
- CS4780: Machine Learning for Intelligent Systems
    - [lecture 20 notes](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote20.pdf)
:::